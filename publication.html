<!DOCTYPE HTML>
<html lang="en">
<head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    gtag('config', 'UA-7580334-2');
  </script>

  <title>Zhi-Qi Cheng</title>
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.2.0/css/bootstrap.min.css">
  
  <meta name="author" content="Zhi-Qi Cheng">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="style/stylesheet.css">
  <link rel="stylesheet" type="text/css" href="style/orig.css" media="all" />
</head>

<body class="blue light">
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:10px;">
      <tbody>
        <tr style="padding:0px">
        <td style="padding:0px">

        <ul class="menu">
            <li class="active"><a href="index.html">Home</a></li>
            <li><a href="publication.html">Publication</a></li>
            <li><a href="teaching.html">Teaching</a></li>
            <li><a href="projects.html">Projects</a></li>     
            <li><a href="talks.html">Talks</a></li>                
            <li><a href="miscellaneous.html">Miscellaneous</a></li>             
        </ul>
        <hr>

        <h3>I: Human Rights & Public Safety: Multimedia Analysis & Assessment</h3>
        <h4>Publications in Conference Proceedings and Journals:</h4>
        <ul>
          <li>
            <strong>Learning spatial awareness to improve crowd counting</strong><br>
            Zhi-Qi Cheng†, Jun-Xiu Li†, Qi Dai, Xiao Wu, Alexander G Hauptmann<br>
            <em>Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2019</em> (Oral Presentation; Acceptance rate: 4.3%)
          </li>
          <li>
            <strong>Rethinking spatial invariance of convolutional networks for object counting</strong><br>
            Zhi-Qi Cheng, Qi Dai, Hong Li, Jingkuan Song, Xiao Wu, Alexander G Hauptmann<br>
            <em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022</em> (Contributed to Pulitzer-winning Capitol Riot analysis by Washington Post 2022; Ranked #2 News Reporting expert on April 15, 2022.)
          </li>
          <li>
            <strong>Improving the learning of multi-column convolutional neural network for crowd counting</strong><br>
            Zhi-Qi Cheng†, Jun-Xiu Li†, Qi Dai, Xiao Wu, Jun-Yan He, Alexander G Hauptmann<br>
            <em>Proceedings of the 27th ACM International Conference on Multimedia (ACM MM), 2019</em> (Oral Presentation)
          </li>
          <li>
            <strong>PoSynDA: Multi-Hypothesis Pose Synthesis Domain Adaptation for Robust 3D Human Pose Estimation</strong><br>
            Hanbing Liu†, Jun-Yan He†, Zhi-Qi Cheng†, Wangmeng Xiang, Qize Yang, Wenhao Chai, Gaoang Wang, Xu Bao, Bin Luo, Yifeng Geng, et al.<br>
            <em>Proceedings of the 31st ACM International Conference on Multimedia (ACM MM), 2023</em>
          </li>
          <li>
            <strong>HDFormer: High-order Directed Transformer for 3D Human Pose Estimation</strong><br>
            Hanyuan Chen†, Jun-Yan He†, Wangmeng Xiang†, Zhi-Qi Cheng†, Wei Liu, Hanbing Liu, Bin Luo, Yifeng Geng, Xuansong Xie<br>
            <em>Proceedings of the 32nd International Joint Conference on Artificial Intelligence (IJCAI), 2023</em>
          </li>
          <li>
            <strong>Crossnet: Boosting crowd counting with localization</strong><br>
            Ji Zhang, Zhi-Qi Cheng, Xiao Wu, Wei Li, Jian-Jun Qiao<br>
            <em>Proceedings of the 30th ACM International Conference on Multimedia (ACM MM), 2022</em>
          </li>
          <li>
            <strong>Stacked pooling for boosting scale invariance of crowd counting</strong><br>
            Siyu Huang, Xi Li, Zhi-Qi Cheng, Zhongfei Zhang, Alexander Hauptmann<br>
            <em>Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2020</em>
          </li>
          <li>
            <strong>DB-LSTM: Densely-connected Bi-directional LSTM for human action recognition</strong><br>
            Jun-Yan He, Xiao Wu, Zhi-Qi Cheng, Zhaoquan Yuan, Yu-Gang Jiang<br>
            <em>Neurocomputing, 2021</em>
          </li>
        </ul>
        <p>† Indicates Equal contribution by authors, ^ indicates Mentorship</p>
        <h4>Other Articles and Preprints</h3>
        <ul>
          <li>
            <strong>Hypergraph transformer for skeleton-based action recognition</strong><br>
            Yuxuan Zhou, Zhi-Qi Cheng^, Chao Li, Yifeng Geng, Xuansong Xie, Margret Keuper<br>
            <em>arXiv preprint arXiv:2211.09590, 2022</em>
          </li>
          <li>
            <strong>Overcoming Topology Agnosticism: Enhancing Skeleton-Based Action Recognition through Redefined Skeletal Topology Awareness</strong><br>
            Yuxuan Zhou, Zhi-Qi Cheng^, Jun-Yan He, Bin Luo, Yifeng Geng, Xuansong Xie, Margret Keuper<br>
            <em>arXiv preprint arXiv:2305.11468, 2023</em>
          </li>
          <li>
            <strong>Refined Temporal Pyramidal Compression-and-Amplification Transformer for 3D Human Pose Estimation</strong><br>
            Hanbing Li†, Wangmeng Xiang†, Jun-Yan He†, Zhi-Qi Cheng†, Bin Luo, Yifeng Geng, Xuansong Xie<br>
            <em>arXiv preprint arXiv:2309.01365, 2023</em>
          </li>
        </ul>
        <p>† Indicates Equal contribution by authors, ^ indicates Mentorship</p>




          
        <ul>
            <li>
              <strong>Instruct-Imagen: Image Generation with Multi-modal Instruction</strong><br>
              Hexiang Hu*, Kelvin C.K. Chan*, Yu-Chuan Su*, <u>Wenhu Chen</u>*, Yandong Li, Kihyuk Sohn, Yang Zhao, Xue Ben, Boqing Gong, William Cohen, Ming-Wei Chang, Xuhui Jia<br>
              <em>Manuscript</em> [<a href="https://arxiv.org/abs/2401.01952">pdf</a>][<a href="https://instruct-imagen.github.io/">website</a>]
            </li>
            <li>
              <strong>VIEScore: Towards Explainable Metrics for Conditional Image Synthesis Evaluation</strong><br>
              Max Ku, Dongfu Jiang, Cong Wei, Xiang Yue, <u>Wenhu Chen</u><br>
              <em>Manuscript</em> [<a href="https://arxiv.org/abs/2312.14867">pdf</a>][<a href="https://tiger-ai-lab.github.io/VIEScore/">website</a>]
            </li>
            <li>
            <strong>MusiLingo: Bridging Music and Text with Pre-trained Language Models for Music Captioning and Query Response</strong><br>
              Zihao Deng, Yinghao Ma, Yudong Liu, Rongchen Guo, Ge Zhang, <u>Wenhu Chen</u>, Wenhao Huang, Emmanouil Benetos<br>
              <em>Manuscript</em> [<a href="https://arxiv.org/abs/2309.08730">pdf</a>][<a href="https://github.com/zihaod/MusiLingo">code</a>]
            </li>
            <li>
            <strong>UniIR: Training and Benchmarking Universal Multimodal Information Retrievers</strong><br>
              Cong Wei, Yang Chen, Haonan Chen, Hexiang Hu, Ge Zhang, Jie Fu, Alan Ritter, <u>Wenhu Chen</u><br>
              <em>Manuscript</em> [<a href="https://arxiv.org/abs/2311.17136">pdf</a>][<a href="https://tiger-ai-lab.github.io/UniIR/">website</a>]
            </li>
            <li>
            <strong>MMMU: A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI</strong><br>
              Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, <u>Wenhu Chen</u><br>
              <em>Manuscript</em> [<a href="https://arxiv.org/abs/2311.16502">pdf</a>][<a href="https://mmmu-benchmark.github.io/">website</a>]
            </li>
            <li>
            <strong>Kosmos-G: Generating Images in Context with Multimodal Large Language Models</strong><br>
              Xichen Pan, Li Dong, Shaohan Huang, Zhiliang Peng, <u>Wenhu Chen</u>, Furu Wei <br>
              <em>Manuscript</em> [<a href="https://arxiv.org/abs/2310.02992">pdf</a>][<a href="https://xichenpan.com/kosmosg/">website</a>]
            </li>
            <li>
            <strong>RoleLLM: Benchmarking, Eliciting, and Enhancing Role-Playing Abilities of Large Language Models </strong><br>
                Zekun Moore Wang, Zhongyuan Peng, Haoran Que, Jiaheng Liu, Wangchunshu Zhou, Yuhan Wu, Hongcheng Guo, Ruitong Gan, Zehao Ni, Man Zhang, Zhaoxiang Zhang, Wanli Ouyang, Ke Xu, <u>Wenhu Chen</u>, Jie Fu, Junran Peng <br>
                <em>Manuscript</em> [<a href="https://arxiv.org/abs/2310.00746">pdf</a>][<a href="https://github.com/InteractiveNLP-Team/RoleLLM-public">code</a>]
            </li>
            <li>
            <strong>TIGERScore: Towards Building Explainable Metric for All Text Generation Tasks </strong><br>
                Dongfu Jiang*, Yishan Li*, Ge Zhang, Wenhao Huang, Bill Yuchen Lin, <u>Wenhu Chen</u> <br>
                <em>Manuscript</em> [<a href="https://arxiv.org/abs/2310.00752">pdf</a>][<a href="https://tiger-ai-lab.github.io/TIGERScore/">website</a>]
            </li>
            <li>
            <strong>ImagenHub: Standardizing the evaluation of conditional image generation models</strong><br>
                Max Ku, Tianle Li, Kai Zhang, Yujie Lu, Xingyu Fu, Wenwen Zhuang, <u>Wenhu Chen</u> <br>
                <em>Manuscript</em> [<a href="https://arxiv.org/abs/2310.01596">pdf</a>][<a href="https://tiger-ai-lab.github.io/ImagenHub/">website</a>]
            </li>
            <li>
            <strong>MAmmoTH: Building Math Generalist Models through Hybrid Instruction Tuning</strong><br>
                Xiang Yue*, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, <u>Wenhu Chen</u>* <br>
                <em>Manuscript</em> [<a href="https://arxiv.org/abs/2309.05653">pdf</a>][<a href="https://tiger-ai-lab.github.io/MAmmoTH/">website</a>]
            </li>
            <li>
            <strong>Augmenting Black-box LLMs with Medical Textbooks for Clinical Question Answering</strong><br>
                Yubo Wang, Xueguang Ma, <u>Wenhu Chen</u> <br>
                <em>Manuscript</em> [<a href="https://arxiv.org/abs/2309.02233">pdf</a>]
            </li>
            <li>
                <strong>MERT: Acoustic Music Understanding Model with Large-Scale Self-supervised Training</strong><br>
                Yizhi Li, Ruibin Yuan, Ge Zhang, Yinghao Ma, Xingran Chen, Hanzhi Yin, Chenghua Lin, Anton Ragni, Emmanouil Benetos, Norbert Gyenge, Roger Dannenberg, Ruibo Liu, <u>Wenhu Chen</u>, Gus Xia, Yemin Shi, Wenhao Huang, Yike Guo, Jie Fu <br>
                <em>Manuscript</em> [<a href="https://arxiv.org/abs/2306.00107">pdf</a>][<a href="https://github.com/yizhilll/MERT">code and data</a>]
            </li>
            </ul>

            <h3> Publications </h3>
            <ul>
            <h4><u>2024</u></h4>
            <li>
                <strong>Explanations from Large Language Models Make Small Reasoners Better</strong><br>
                Shiyang Li, Jianshu Chen, yelong shen, Zhiyu Chen, Xinlu Zhang, Zekun Li, Hong Wang, Jing Qian, Baolin Peng, Yi Mao, <u>Wenhu Chen</u>, Xifeng Yan <br>
                <em>SAI-AAAI 2024, Vancouver, Canada</em> [<a href="https://arxiv.org/abs/2210.06726">pdf</a>]
            </li>
            <li>
                <strong> Interactive Natural Language Processing </strong><br>
                Zekun Wang, Ge Zhang, Kexin Yang, Ning Shi, Wangchunshu Zhou, Shaochun Hao, Guangzheng Xiong, Yizhi Li, Mong Yuan Sim, Xiuying Chen, Qingqing Zhu, Zhenzhu Yang, Adam Nik, Qi Liu, Chenghua Lin, Shi Wang, Ruibo Liu, <u>Wenhu Chen</u>, Ke Xu, Dayiheng Liu, Yike Guo, Jie Fu <br>
                <em>Springer Nature</em> [<a href="https://arxiv.org/abs/2305.13246">pdf</a>]
            </li>
            <li>
                <strong> Synthesizing Coherent Story with Auto-Regressive Latent Diffusion Models </strong><br>
                Xichen Pan, Pengda Qin, Yuhong Li, Hui Xue, <u>Wenhu Chen</u> <br>
                <em>Proceedings of WACV 2024, Hawaii, USA</em> [<a href="https://arxiv.org/abs/2211.10950">pdf</a>][<a href="https://github.com/Flash-321/ARLDM">code</a>]<br>
                (<font color='red'>Oral Presentation</font>)
            </li>
            <h4><u>2023</u></h4>
            <li>
            <strong>DreamEdit: Subject-driven Image Editing</strong><br>
                Tianle Li, Max Ku, Cong Wei, <u>Wenhu Chen</u> <br>
                <em>TMLR 2023</em> [<a href="https://arxiv.org/abs/2306.12624">pdf</a>][<a href="https://dreameditbenchteam.github.io/">website</a>]
            </li>
            <li>
            <strong>Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks </strong><br>
                <u>Wenhu Chen*</u>, Xueguang Ma*, Xinyi Wang, William W. Cohen<br>
                <em>TMLR 2023</em> [<a href="https://arxiv.org/abs/2211.12588">pdf</a>][<a href="https://github.com/wenhuchen/Program-of-Thoughts">code</a>]
            </li>
            <li>
            <strong>On the Risk of Misinformation Pollution with Large Language Models</strong><br>
                Yikang Pan, Liangming Pan, <u>Wenhu Chen</u>, Preslav Nakov, Min-Yen Kan, William Yang Wang <br>
                <em>Findings of EMNLP 2023, Singapore</em> [<a href="https://arxiv.org/abs/2305.13661">pdf</a>]
            </li>
            <li>
            <strong>EDIS: Entity-Driven Image Search over Multimodal Web Content</strong><br>
                Siqi Liu, Weixi Feng, <u>Wenhu Chen</u>, William Yang Wang <br>
                <em>Proceedings of EMNLP 2023, Singapore</em> [<a href="https://arxiv.org/abs/2305.13631">pdf</a>][<a href="https://github.com/emerisly/EDIS">code and data</a>]
            </li>
            <li>
                <strong> TheoremQA: A Theorem-driven Question Answering dataset </strong><br>
                <u>Wenhu Chen</u>, Ming Yin, Max Ku, Elaine Wan, Xueguang Ma, Jianyu Xu, Tony Xia, Xinyi Wang, Pan Lu <br>
                <em>Proceedings of EMNLP 2023, Singapore</em> [<a href="https://arxiv.org/abs/2305.12524">pdf</a>][<a href="https://github.com/wenhuchen/TheoremQA">code and data</a>]
            </li>
            <li>
                <strong> Subject-driven Text-to-Image Generation via Apprenticeship Learning </strong><br>
                <u>Wenhu Chen</u>, Hexiang Hu, Yandong Li, Nataniel Ruiz, Xuhui Jia, Ming-Wei Chang, William W. Cohen <br>
                <em>Proceedings of NeurIPS 2023, New Orleans, USA</em> [<a href="https://arxiv.org/abs/2304.00186">pdf</a>][<a href="https://open-vision-language.github.io/suti/">website</a>]
            </li>
            <li>
            <strong>MARBLE: Music Audio Representation Benchmark for Universal Evaluation</strong><br>
                Ruibin Yuan, Yinghao Ma, Yizhi Li, Ge Zhang, Xingran Chen, Hanzhi Yin, Le Zhuo, Yiqi Liu, Jiawen Huang, Zeyue Tian, Binyue Deng, Ningzhi Wang, <u>Wenhu Chen</u>, Gus Xia, Wei Xue, Si Liu, Shi Wang, Ruibo Liu, Yike Guo, Jie Fu <br>
                <em>Proceedings of NeurIPS 2023, New Orleans, USA</em> [<a href="https://arxiv.org/abs/2306.10548">pdf</a>][<a href="https://marble-bm.shef.ac.uk/">benchmark</a>] 
            </li>
            <li>
            <strong>MagicBrush: A Manually Annotated Dataset for Instruction-Guided Image Editing</strong><br>
                Kai Zhang, Lingbo Mo, <u>Wenhu Chen</u>, Huan Sun, Yu Su<br>
                <em>Proceedings of NeurIPS 2023, New Orleans, USA</em> [<a href="https://arxiv.org/abs/2306.10012">pdf</a>][<a href="https://osu-nlp-group.github.io/MagicBrush/">website</a>]
            </li>
            <li>
                <strong> Attacking Open-domain Question Answering by Injecting Misinformation</strong><br>
                Liangming Pan, <u>Wenhu Chen</u>, Min-Yen Kan and William Yang Wang <br>
                <em>Proceedings of IJCNLP-AACL 2023, Bali, Indonesia</em> [<a href="https://arxiv.org/abs/2110.07803">pdf</a>]<br>
                (<font color='red'>Area Chair Award in Question Answering Track</font>)

            </li>
            <li>
                <strong> LyricWhiz: Robust Multilingual Lyrics Transcription by Whispering to ChatGPT</strong><br>
                Le Zhuo, Ruibin Yuan, Jiahao Pan, Yinghao Ma, Yizhi Li, Ge Zhang, Si Liu, Roger Dannenberg, Jie Fu, Chenghua Lin, Emmanouil Benetos, <u>Wenhu Chen</u>, Wei Xue, Yike Guo <br>
                <em>Proceedings of ISMIR 2023, Milan, Italy</em> [<a href="https://arxiv.org/abs/2306.17103">pdf</a>]
            </li>
            <li>
                <strong> DePlot: One-shot visual language reasoning by plot-to-table translation </strong><br>
                Fangyu Liu, Julian Martin Eisenschlos, Francesco Piccinno, Syrine Krichene, Chenxi Pang, Kenton Lee, Mandar Joshi, <u>Wenhu Chen</u>, Nigel Collier, Yasemin Altun <br>
                <em>Findings of ACL 2023, Toronto, Canada</em> [<a href="https://arxiv.org/abs/2212.10505">pdf</a>] [<a href="https://huggingface.co/google/deplot">demo</a>]
            </li>
            <li>
                <strong> Few-shot In-context Learning on Knowledge Base Question Answering </strong><br>
                Tianle Li, Xueguang Ma, Alex Zhuang, Yu Gu, Yu Su and <u>Wenhu Chen</u> <br>
                <em>Proceedings of ACL 2023, Toronto, Canada</em> [<a href="https://arxiv.org/abs/2305.01750">pdf</a>][<a href="https://github.com/ltl3A87/KB-BINDER">code</a>]        
            </li>
            <li>
                <strong> Large Language Models are few(1)-shot Table Reasoners </strong><br>
                <u>Wenhu Chen</u><br>
                <em>Findings of EACL 2023, Dubrovnik, Croatia</em> [<a href="https://arxiv.org/abs/2210.06710">pdf</a>][<a href="https://github.com/wenhuchen/TableCoT">code</a>]
            </li>
            <li>
                <strong> Augmenting Pre-trained Language Models with QA-Memory for Open-Domain Question Answering </strong><br>
                <u>Wenhu Chen</u>, Pat Verga, Michiel de Jong, John Wieting, William W. Cohen <br>
                <em>Proceedings of EACL 2023, Dubrovnik, Croatia</em> [<a href="https://arxiv.org/abs/2204.04581">pdf</a>]
            </li>
            <li>
                <strong> Re-Imagen: Retrieval-Augmented Text-to-Image Generator </strong><br>
                <u>Wenhu Chen</u>,Hexiang Hu, Chitwan Saharia, William W. Cohen <br>
                <em>Proceedings of ICLR 2023, Kigali, Rwanda </em> [<a href="https://arxiv.org/abs/2209.14491">pdf</a>]
            </li>
            <h4><u>2022</u></h4>
            <li>
                <strong> QA Is the New KR: Question-Answer Pairs as Knowledge Bases </strong><br>
                William W. Cohen, <u>Wenhu Chen</u>, Michiel De Jong, Nitish Gupta, Alessandro Presta, Pat Verga, John Wieting (All Equal Contribution) <br>
                <em>Proceedings of AAAI 2023, Senior Member Track</em> [<a href="https://arxiv.org/abs/2207.00630">pdf</a>]
            </li>
            <li>
                <strong> MuRAG: Multimodal Retrieval-Augmented Generator for Open Question Answering over Images and Text </strong><br>
                <u>Wenhu Chen</u>, Hexiang Hu, Xi Chen, Pat Verga, William W. Cohen <br>
                <em>Proceedings of EMNLP 2022, Abu Dhabi </em> [<a href="https://arxiv.org/abs/2210.02928">pdf</a>]
            </li>
            <li>
                <strong> Controllable Dialogue Simulation with In-context Learning </strong><br>
                Zekun Li, <u>Wenhu Chen</u>, Shiyang Li, Hong Wang, Jing Qian and Xifeng Yan <br>
                <em>Findings of EMNLP 2022, Abu Dhabi </em> [<a href="https://arxiv.org/abs/2210.04185">pdf</a>]
            </li>
            <li>
                <strong> HybriDialogue: An Information-Seeking Dialogue Dataset Grounded on Tabular and Textual Data </strong><br>
                Kai Nakamura, Sharon Levy, Yi-Lin Tuan, <u>Wenhu Chen</u>, William Yang Wang <br>
                <em>Findings of ACL 2022, Virtual</em> [<a href="https://arxiv.org/abs/2204.13243">pdf</a>][<a href="https://github.com/entitize/HybridDialogue">code</a>] 
            </li>
            <h4><u>2021</u></h4>
            <li>
                <strong> A Dataset for Answering Time-Sensitive Questions </strong><br>
                <u>Wenhu Chen</u>, Xinyi Wang, William Yang Wang <br>
                <em>Proceedings of NeurIPS 2021, Virtual</em> [<a href="http://arxiv.org/abs/2108.06314">pdf</a>][<a href="https://github.com/wenhuchen/Time-Sensitive-QA">code</a>] 
            </li>
            <li>
                <strong> Local Explanation of Dialogue Response Generation </strong><br>
                Yi-Lin Tuan, Connor Pryor, <u>Wenhu Chen</u>, Lise Getoor, William Yang Wang <br>
                <em>Proceedings of NeurIPS 2021, Virtual</em> [<a href="https://arxiv.org/abs/2106.06528">pdf</a>][<a href="https://github.com/Pascalson/LERG">code</a>]
            </li>
            <li>
                <strong> Counterfactual Maximum Likelihood Estimation for Training Deep Networks </strong><br>
                Xinyi Wang, <u>Wenhu Chen</u>, Michael Saxon, William Yang Wang <br>
                <em>Proceedings of NeurIPS 2021, Virtual</em> [<a href="https://arxiv.org/abs/2106.03831">pdf</a>][<a href="https://github.com/WANGXinyiLinda/CMLE">code</a>]
            </li>
            <li>
                <strong> FinQA: A Dataset of Numerical Reasoning over Financial Data </strong><br>
                Zhiyu Chen, <u>Wenhu Chen</u>, Charese Smiley, Sameena Shah, Iana Borova, Dylan Langdon, \\ <br>
                Reema Moussa, Matt Beane, Ting-Hao Huang, Bryan R. Routledge, William Yang Wang <br>
                <em>Proceedings of EMNLP 2021, Virtual</em> [<a href="https://arxiv.org/abs/2109.00122">pdf</a>][<a href="https://github.com/czyssrs/finqa">code</a>]
            </li>  
            <li>
                <strong> Task-adaptive Pre-training and Self-training are Complementary for Natural Language Understanding </strong><br>
                Shiyang Li, Semih Yavuz, <u>Wenhu Chen</u> and Xifeng Yan <br>
                <em>Findings of EMNLP 2021, Virtual</em> [<a href="https://arxiv.org/abs/2109.06466">pdf</a>]
            </li>  
            <li>
                <strong> Zero-shot Fact Verification by Claim Generation </strong><br>
                Liangming Pan, <u>Wenhu Chen</u>, Wenhan Xiong, Min-Yen Kan and William Wang <br>
                <em>Proceedings of ACL 2021, Virtual</em> [<a href="https://arxiv.org/abs/2105.14682">pdf</a>][<a href="https://github.com/teacherpeterpan/Zero-shot-Fact-Verification">code</a>]
            </li>
            <li>
                <strong> A Systematic Investigation of KB-Text Embedding Alignment at Scale </strong><br>
                Vardaan Pahuja, Yu Gu, <u>Wenhu Chen</u>, Mehdi Bahrami, Lei Liu, Wei-Peng Chen and Yu Su<br>
                <em>Proceedings of ACL 2021, Virtual</em> [<a href="https://arxiv.org/pdf/2106.01586.pdf">pdf</a>][<a href="https://github.com/dki-lab/joint-kb-text-embedding">code</a>]
            </li>
            <li>
                <strong> Unsupervised Multi-hop Question Answering by Question Generation </strong><br>
                Liangming Pan, <u>Wenhu Chen</u>, Wenhan Xiong, Min-Yen Kan, William Wang <br>
                <em>Proceedings of NAACL 2021, Mexico City, Mexico</em> [<a href="https://arxiv.org/abs/2010.12623">pdf</a>][<a href='https://github.com/teacherpeterpan/Unsupervised-Multi-hop-QA'>code</a>]
            </li>
            <li>
                <strong> Open Question Answering over Tables and Text </strong><br>
                <u>Wenhu Chen</u>, Ming-wei Chang, Eva Schlinger, William Wang, William Cohen <br>
                <em>Proceedings of ICLR 2021, Virtual</em> [<a href="https://arxiv.org/abs/2010.10439">pdf</a>][<a href="https://github.com/wenhuchen/OTT-QA">data and code</a>] [<a href="https://competitions.codalab.org/competitions/27324">codalab</a>][<a href="images/ott-qa.pptx">slides</a>][<a href="images/ICLR-talk.pptx">talk-slides</a>]
            </li>        
            <li>
                <strong> Meta Module Network for Compositional Visual Reasoning </strong><br>
                <u>Wenhu Chen</u>, Zhe Gan, Linjie Li, Yu Cheng, William Wang, Jingjing Liu <br>
                <em>Proceedings of WACV 2021, Hawaii, USA </em> [<a href="https://arxiv.org/abs/1910.03230">pdf</a>][<a href="https://github.com/wenhuchen/Meta-Module-Network">code</a>]<br>
                (<font color='red'>Best Student Paper Honorable Mention</font>)
            </li>

            <h4><u>2020</u></h4>
            <li>
                <strong> KGPT: Knowledge-Grounded Pre-Training for Data-to-Text Generation </strong><br>
                <u>Wenhu Chen</u>, Yu Su, Xifeng Yan, William Wang <br>
                <em>Proceedings of EMNLP 2020, Punta Cana, Dominican</em> [<a href="https://arxiv.org/abs/2010.02307">pdf</a>][<a href="https://github.com/wenhuchen/KGPT">data and code</a>][<a href="images/EMNLP-Oral.pptx">slides</a>]
            </li>
            <li>
                <strong> HybridQA: A Dataset of Multi-Hop Question Answering over Tabular and Textual Data
                </strong><br>
                <u>Wenhu Chen</u>, Hanwen Zha, Zhiyu Chen, Wenhan Xiong, Hong Wang, William Wang <br>
                <em>Findings of EMNLP 2020</em> [<a href="https://arxiv.org/abs/2004.07347">pdf</a>][<a href="https://github.com/wenhuchen/HybridQA">data and code</a>][<a href="https://competitions.codalab.org/competitions/24420">codalab</a>][<a href="https://hybridqa.github.io/">website</a>]
            </li>
            <li>
                <strong> Logic2Text: High-Fidelity Natural Language Generation from Logical Forms
             </strong><br>
                Zhiyu Chen, <u>Wenhu Chen</u>, Hanwen Zha, Xiyou Zhou, Yunkai Zhang, Sairam Sundaresan, William Wang <br>
                <em>Findings of EMNLP 2020</em> [<a href="https://arxiv.org/abs/2004.14579">pdf</a>][<a href="https://github.com/czyssrs/Logic2Text">data and code</a>]
            </li>
            <li>
                <strong> Logical Natural Language Generation from Open-Domain Tables </strong><br>
                <u>Wenhu Chen</u>, Jianshu Chen, Yu Su, Zhiyu Chen and William Wang<br>
                <em>Proceedings of ACL 2020, Seattle, USA</em> [<a href="https://arxiv.org/abs/2004.10404">pdf</a>][<a href="https://github.com/wenhuchen/LogicNLG">data and code</a>][<a href="https://competitions.codalab.org/competitions/24527">codalab</a>][<a href="images/ACL2020-Oral.pptx">slides</a>]
            </li>
            <li>
                <strong> Few-shot NLG with Pre-trained Language Model </strong><br>
                Zhiyu Chen, Harini Eavani, <u>Wenhu Chen</u>, Yinyin Liu and William Wang<br>
                <em>Proceedings of ACL 2020, Seattle, USA</em> [<a href="https://arxiv.org/abs/1904.09521">pdf</a>][<a href="https://github.com/czyssrs/Few-Shot-NLG">data and code</a>]
            </li>
            <li>
                <strong> VIOLIN: A Large-Scale Dataset for Video-and-Language Inference </strong><br>
                Jingzhou Liu, <u>Wenhu Chen</u>, Yu Cheng, Zhe Gan, Licheng Yu, Yiming Yang, Jingjing Liu<br>
                <em>Proceedings of CVPR 2020, Seattle, USA</em> [<a href="https://arxiv.org/abs/2003.11618">pdf</a>][<a href="https://github.com/jimmy646/violin">data and code</a>]
            </li>
            <li>
                <strong> TabFact: A Large-scale Dataset for Table-based Fact Verification </strong><br>
                <u>Wenhu Chen</u>, Hongmin Wang, Jianshu Chen, Yunkai Zhang, Hong Wang, Shiyang Li, Xiyou Zhou and William Wang<br>
                <em>Proceedings of ICLR 2020, Addis Ababa, Ethiopia</em> [<a href="https://arxiv.org/abs/1909.02164">pdf</a>][<a href="https://tabfact.github.io/">website</a>][<a href="https://github.com/wenhuchen/Table-Fact-Checking">data and code</a>][<a href="https://competitions.codalab.org/competitions/21611">codalab</a>][<a href="images/ICLR2020-Oral.pptx">slides</a>]
            </li>
            <li>
                <strong> Generative Adversarial Zero-Shot Relational Learning for Knowledge Graphs </strong><br>
                Pengda Qin, Xin Wang, <u>Wenhu Chen</u>, Chunyun Zhang, Weiran Xu, William Wang<br>
                <em>Proceedings of AAAI 2020, New York, US (Oral)</em> [<a href="https://arxiv.org/abs/2001.02332">pdf</a>][<a href="https://blog.csdn.net/qq_38382642/article/details/104109825">blog</a>][<a href="https://github.com/Panda0406/Zero-shot-knowledge-graph-relational-learning">code</a>]
            </li>

            <h4><u>2019</u></h4>
            <li>
                <strong> Enhancing the Locality and Breaking the Memory Bottleneck of Transformer on Time Series Forecasting </strong><br>
                Shiyang Li, Xiaoyong Jin, Yao Xuan, Xiyou Zhou, <u>Wenhu Chen</u>, Yu-Xiang Wang, Xifeng Yan<br>
                <em>Proceedings of NeurIPS 2019, Vancouver, Canada</em> [<a href="https://arxiv.org/abs/1907.00235">pdf</a>]
            </li>
            <li>
                <strong> Interpreting and Improving Deep Neural SLU Models via Vocabulary Importance  </strong><br>
                Yilin Shen, <u>Wenhu Chen</u>, Hongxia Jin<br>
                <em>Proceedings of INTERSPEECH 2019, Graz, Austria (Oral)</em> [<a href="https://www.isca-speech.org/archive/Interspeech_2019/pdfs/3184.pdf">pdf</a>]
            </li>
            <li>
                <strong> Semantically Conditioned Dialog Response Generation via Hierarchical Disentangled Self-Attention </strong><br>
                <u>Wenhu Chen</u>, Jianshu Chen, Pengda Qin, Xifeng Yan and William Wang<br>
                <em>Proceedings of ACL 2019, Florence, Italy</em> [<a href="https://arxiv.org/abs/1905.12866">pdf</a>][<a href="https://github.com/wenhuchen/HDSA-Dialog">code</a>]
            </li>
            <li>
                <strong> Global Textual Relation Embedding for Relational Understanding </strong><br>
                Zhiyu Chen, Hanwen Zha, Honglei Liu, <u>Wenhu Chen</u>, Xifeng Yan and Yu Su<br>
                <em>Proceedings of ACL 2019, Florence, Italy </em> [<a href="https://arxiv.org/abs/1906.00550">pdf</a>][<a href="https://github.com/czyssrs/GloREPlus">code</a>]
            </li>
            <li>
                <strong> Mining Algorithm Roadmap in Scientific Publications </strong><br>
                Hanwen Zha, <u>Wenhu Chen</u>, Keqian Li and Xifeng Yan<br>
                <em>Proceedings of KDD 2019, Alaska, USA (Oral)</em> [<a href="https://dl.acm.org/citation.cfm?id=3330913">pdf</a>][<a href="https://www.aminer.cn/research_report/5d70705aba8f0e1dd32ca39a?download=false">blog</a>][<a href="https://www.youtube.com/watch?v=_gguTc7UK6k">video</a>][<a href="https://github.com/zhw12/AlgMap">code</a>]
            </li>
            <li>
                <strong> How Large A Vocabulary Does Text Classification Need? A Variational Approach on Vocabulary Selection  </strong><br>
                <u>Wenhu Chen</u>, Yu Su, Yilin Shen, Zhiyu Chen, Xifeng Yan and William Wang<br>
                <em>Proceedings of NAACL 2019, Minneapolis, USA (Oral)</em> [<a href="https://arxiv.org/abs/1902.10339">pdf</a>][<a href="https://github.com/wenhuchen/Variational-Vocabulary-Selection.git">code</a>][<a href='images/naacl2019.pptx'>slides</a>]
            </li>
            <h4><u>Before 2019</u></h4>
            <li><strong> XL-NBT: A cross-lingual Neural Belief Tracking Framework  </strong><br>
                    <u>Wenhu Chen</u>, Jianshu Chen, Yu Su, Xin Wang, Dong Yu, Xifeng Yan and William Wang.<br>
                    <em>Proceedings of EMNLP 2018, Brussels, Belgium </em> [<a href="https://arxiv.org/abs/1808.06244">pdf</a>][<a href="https://github.com/wenhuchen/Cross-Lingual-NBT">code</a>][<a href="https://www.jiqizhixin.com/articles/2018-10-25-4">blog</a>]
            </li>
            <li><strong> Triangular Architecture for Rare Language Translation </strong><br>
                    Shuo Ren, <u>Wenhu Chen</u>, Shujie Liu, Mu Li, Ming Zhou and Shuai Ma.<br>
                    <em>Proceedings of ACL 2018, Melbourne, Australia (Oral)</em> [<a href="https://arxiv.org/abs/1805.04813">pdf</a>][<a href="images/TA-NMT.pptx">slides</a>][<a href="https://www.msra.cn/zh-cn/news/features/acl-2018-ta-nmt">blog</a>]
            </li>
            <li><strong> No Metrics Are Perfect: Adversarial Reward Learning for Visual Storytelling</strong><br>
                    <u>Wenhu Chen</u>*, Xin Wang*, Yuan-Fang Wang and William Wang (*Equal)<br>
                    <em>Proceedings of ACL 2018, Melbourne, Australia (Oral)</em> [<a href="https://arxiv.org/abs/1804.09160">pdf</a>][<a href="https://github.com/littlekobe/AREL">code</a>][<a href="https://www.leiphone.com/news/201804/b8Nq00SkvuBTrkaq.html">blog</a>][<a href="images/308_Poster.pdf">slides</a>]
            </li>
            <li><strong> Variational Knowledge Graph Reasoning </strong><br>
                    <u>Wenhu Chen</u>, Wenhan Xiong, Xifeng Yan, William Wang.<br>
                    <em>Proceedings of NAACL 2018, New Orleans, CA (Oral) </em> [<a href="https://arxiv.org/abs/1803.06581">pdf</a>][<a href="images/naacl2018.pptx">slides</a>][<a href="https://github.com/wenhuchen/KB-Reasoning-Data">data</a>][<a href="http://tech.ifeng.com/a/20180328/44921977_0.shtml">blog</a>][<a href="https://vimeo.com/277673049">video</a>]
            </li>

            <li><strong> Generative Bridging Network in Neural Sequence Prediction </strong><br>
                <u>Wenhu Chen</u>, Guanlin Li, Shuo Ren, Shujie Liu, Zhirui Zhang, Mu Li, Ming Zhou.<br>
                <em>Proceedings of NAACL 2018, New Orleans, CA </em> [<a href="https://arxiv.org/abs/1706.09152">pdf</a>][<a href="images/naacl_poster_GBN.pdf">poster</a>]
            </li>

            <li><strong> Video Captioning via Hierarchical Reinforcement Learning  </strong><br>
            Xin Wang, <u>Wenhu Chen</u>, Jiawei Wu, Yuan-fang Wang, William Wang.<br>
            <em>Proceedings of CVPR 2018, Salt Lake City, UTAH </em> [<a href="https://arxiv.org/pdf/1711.11135.pdf">pdf</a>][<a href="https://cloud.tencent.com/developer/article/1092810">blog</a>]
            </li>

            <li><strong> Guided alignment training for topic-aware neural machine translation </strong><br>
            <u>Wenhu Chen</u>, Evgeny Matusov, Shahram Khadivi, JT Peter.<br>
            <em>Proceedings of AMTA 2016, Austin, TX (Oral) </em> [<a href="https://arxiv.org/pdf/1607.01628.pdf">pdf</a>][<a href="https://github.com/harvardnlp/seq2seq-attn">openNMT</a>]
            </li>

            </ul>
            <hr>
            
            <h3> Thesis </h3>
            <ul>
            <li>
            <strong> Accessing Diverse Web Knowledge with Natural Language Interface </strong><br>
            <u>Wenhu Chen</u> <br>
            <em>PhD dissertation at University of California, Santa Barbara</em> [<a href="https://escholarship.org/uc/item/76x242b1">pdf</a>]
            </li>
            </ul>

        </td>
        </tr>
      </tbody>
    </table>
</body>
